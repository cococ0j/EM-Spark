{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EM algorithm with spark vs without spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from EM import* \n",
    "from time import time\n",
    "from generate import GM_sample\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computation time and comparison: \n",
    "\n",
    "We already simulated a set of GMM data for different values of (N,k,d). We are going to run EM with spark and without spark for several times (random initializations) for each triplet (N,k,d). We do not investigate the effect of N specifically but rather study the importance of mapreduce in terms of computational time: \n",
    "\n",
    "<font color=\"red\"> Takes a lot of time ! </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_N_K_d(X,y):\n",
    "    \"\"\" Get N, K and d given the data X and the true clusters vector y \"\"\"  \n",
    "    return X.shape[0], len(np.unique(y)), X.shape[1]\n",
    "\n",
    "labels = [100,200,500,1000,2000,5000,10000]\n",
    "n_labels = len(labels)\n",
    "\n",
    "max_iter = 25\n",
    "tol = -1\n",
    "sample_size = 10\n",
    "\n",
    "time_spark = np.zeros((n_labels,sample_size))\n",
    "time_nospark = np.zeros((n_labels,sample_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i,label in enumerate(labels):\n",
    "    print(i)\n",
    "    for j in range(sample_size):\n",
    "        X = np.load(\"Datasets/dataset_\"+str(label)+\"_X.npy\")\n",
    "        y = np.load(\"Datasets/dataset_\"+str(label)+\"_y.npy\")\n",
    "        N,K,d = get_N_K_d(X,y)\n",
    "        rdd = sc.parallelize(X)\n",
    "        \n",
    "        #NO SPARK\n",
    "        EM_ = EM_noSpark()\n",
    "        t = time()\n",
    "        EM_.fit(X, n_clusters = K, max_iter = max_iter, criterion = None, verbose = False, tol=-1)\n",
    "        time_nospark[i,j] = time()-t\n",
    "        \n",
    "        #SPARK\n",
    "        EM = EM_Spark()\n",
    "        t = time()\n",
    "        EM.fit(rdd, n_clusters = K, max_iter = max_iter, criterion = None, verbose = False, tol=-1)\n",
    "        time_spark[i,j] = time()-t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#np.save(\"time_nospark\",time_nospark)\n",
    "#np.save(\"time_spark\",time_spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.scatter(np.repeat(labels,sample_size), time_spark, c=\"r\",label=\"Spark\")\n",
    "plt.scatter(np.repeat(labels,sample_size), time_nospark, c=\"b\",label=\"No Spark\")\n",
    "plt.legend();"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
